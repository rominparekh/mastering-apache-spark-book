== DataFrameWriter

`DataFrameWriter` is an interface to persist a link:spark-sql-dataset.adoc[Dataset] to an external storage system in a batch fashion.

TIP: Read link:spark-sql-streaming-DataStreamWriter.adoc[DataStreamWriter] for streamed writing.

You use link:spark-sql-dataframe.adoc#write[write] method on a `Dataset` to access `DataFrameWriter`.

[source, scala]
----
import org.apache.spark.sql.DataFrameWriter

val nums: Dataset[Long] = ...
val writer: DataFrameWriter[Row] = nums.write
----

`DataFrameWriter` has a direct support for many <<writing-dataframes-to-files, file formats>>, <<jdbc, JDBC databases>> and <<format, an interface to plug in new formats>>. It assumes <<parquet, parquet>> as the default data source that you can change using link:spark-sql-settings.adoc[spark.sql.sources.default] setting or <<format, format>> method.

[source, scala]
----
// see above for writer definition

// Save dataset in Parquet format
writer.save(path = "nums")

// Save dataset in JSON format
writer.format("json").save(path = "nums-json")
----

In the end, you trigger the actual saving of the content of a `Dataset` using <<save, save>> method.

[source, scala]
----
writer.save
----

NOTE: Interestingly, a `DataFrameWriter` is really a type constructor in Scala. It keeps a reference to a source `DataFrame` during its lifecycle (starting right from the moment it was created).

=== [[internal-state]] Internal State

`DataFrameWriter` uses the following mutable attributes to build a properly-defined write specification for <<insertInto, insertInto>>, <<saveAsTable, saveAsTable>>, and <<save, save>>:

.Attributes and Corresponding Setters
[cols="1,2",options="header"]
|===
|Attribute |Setters
|`source`        |<<format, format>>
|`mode` | <<mode, mode>>
|`extraOptions` | <<option, option>>, <<options, options>>, <<save, save>>
|`partitioningColumns` | <<partitionBy, partitionBy>>
|`bucketColumnNames` | <<bucketBy, bucketBy>>
|`numBuckets` | <<bucketBy, bucketBy>>
|`sortColumnNames` | <<sortBy, sortBy>>
|===

=== [[saveAsTable]] `saveAsTable` Method

[source, scala]
----
saveAsTable(tableName: String): Unit
----

`saveAsTable` saves the content of a `DataFrame` as the `tableName` table.

First, `tableName` is parsed to an internal table identifier. `saveAsTable` then checks whether the table exists or not and uses <<mode, save mode>> to decide what to do.

`saveAsTable` uses the link:spark-sql-SessionCatalog.adoc[SessionCatalog] for the current session.

.``saveAsTable``'s Behaviour per Save Mode
[cols="1,1,2",options="header"]
|===
| Does table exist? |Save Mode | Behaviour
| yes       | `Ignore` | Do nothing
| yes       | `ErrorIfExists` | Throws a `AnalysisException` exception with `Table [tableIdent] already exists.` error message.
| _anything_       | _anything_ | It creates a `CatalogTable` and link:spark-sql-sessionstate.adoc#executePlan[executes the `CreateTable` plan].
|===

[source, scala]
----
val ints = 0 to 9 toDF
val options = Map("path" -> "/tmp/ints")
ints.write.options(options).saveAsTable("ints")
sql("show tables").show
----

=== [[save]] Persisting DataFrame -- `save` Method

[source, scala]
----
save(): Unit
----

`save` saves the content of a `Dataset` to...FIXME

Internally, `save` first checks whether the `DataFrame` is not bucketed.

CAUTION: FIXME What does `bucketing` mean?

`save` then creates a link:spark-sql-datasource.adoc[DataSource] (for the `source`) and calls link:spark-sql-datasource.adoc#write[write] on it.

NOTE: `save` uses `source`, `partitioningColumns`, `extraOptions`, and `mode` internal attributes directly. They are specified through the API.

=== [[jdbc]] `jdbc` Method

[source, scala]
----
jdbc(url: String, table: String, connectionProperties: Properties): Unit
----

`jdbc` method saves the content of the `DataFrame` to an external database table via JDBC.

You can use <<mode, mode>> to control *save mode*, i.e. what happens when an external table exists when `save` is executed.

It is assumed that the `jdbc` save pipeline is not <<partitionBy, partitioned>> and <<bucketBy, bucketed>>.

All <<options, options>> are overriden by the input `connectionProperties`.

The required options are:

* `driver` which is the class name of the JDBC driver (that is passed to Spark's own `DriverRegistry.register` and later used to `connect(url, properties)`).

When `table` exists and the <<mode, override save mode>> is in use, `DROP TABLE table` is executed.

It creates the input `table` (using `CREATE TABLE table (schema)` where `schema` is the schema of the `DataFrame`).

=== [[bucketBy]] `bucketBy` Method

CAUTION: FIXME

=== [[partitionBy]] `partitionBy` Method

[source, scala]
----
partitionBy(colNames: String*): DataFrameWriter[T]
----

CAUTION: FIXME

=== [[mode]] Specifying Save Mode -- `mode` Method

[source, scala]
----
mode(saveMode: String): DataFrameWriter[T]
mode(saveMode: SaveMode): DataFrameWriter[T]
----

You can control the behaviour of write using `mode` method, i.e. what happens when an external file or table exist when `save` is executed.

* `SaveMode.Ignore` or
* `SaveMode.ErrorIfExists` or
* `SaveMode.Overwrite` or

=== [[option]][[options]] Writer Configuration -- `option` and `options` Methods

CAUTION: FIXME

=== [[writing-dataframes-to-files]] Writing DataFrames to Files

CAUTION: FIXME

=== [[format]] Specifying Alias or Fully-Qualified Class Name of DataSource -- `format` Method

CAUTION: FIXME Compare to DataFrameReader.

=== [[parquet]] Parquet

CAUTION: FIXME

NOTE: Parquet is the default data source format.

=== [[insertInto]] `insertInto` Method

CAUTION: FIXME
