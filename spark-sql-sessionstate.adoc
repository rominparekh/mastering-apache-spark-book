== [[SessionState]] SessionState

`SessionState` is the <<sessionState, state separation layer>> between sessions, including SQL configuration, tables, functions, UDFs, the link:spark-sql-sql-parsers.adoc#SparkSqlParser[SQL parser], and everything else that depends on a link:spark-sql-SQLConf.adoc[SQLConf].

It uses a link:spark-sql-sparksession.adoc[SparkSession] and manages its own link:spark-sql-SQLConf.adoc[SQLConf].

NOTE: Given the package `org.apache.spark.sql.internal` that `SessionState` belongs to, this one is truly _internal_. You've been warned.

NOTE: `SessionState` is a `private[sql]` class.

`SessionState` offers the following services:

* <<optimizer, optimizer>>
* <<analyzer, analyzer>>
* <<catalog, catalog>>
* <<streamingQueryManager, streamingQueryManager>>
* <<udf, udf>>
* <<newHadoopConf, newHadoopConf>> to create a new Hadoop's `Configuration`.
* link:spark-sql-sparksession.adoc#sessionState[sessionState]
* link:spark-sql-sql-parsers.adoc#SparkSqlParser[sqlParser]
* <<executePlan, executePlan>>

=== [[catalog]] `catalog` Attribute

[source, scala]
----
catalog: SessionCatalog
----

`catalog` attribute points at shared internal link:spark-sql-SessionCatalog.adoc[SessionCatalog] for managing tables and databases.

It is used to create the shared <<analyzer, analyzer>>, <<optimizer, optimizer>>

=== [[analyzer]] `analyzer` Attribute

[source, scala]
----
analyzer: Analyzer
----

`analyzer` is the link:spark-sql-catalyst-analyzer.adoc[Analyzer] for the current Spark SQL session.

=== [[optimizer]] Accessing Catalyst Query Optimizer -- `optimizer` Attribute

[source, scala]
----
optimizer: Optimizer
----

`optimizer` is a Spark session's link:spark-sql-catalyst-Optimizer.adoc[Catalyst query optimizer] for link:spark-sql-LogicalPlan.adoc[logical query plans].

It is (lazily) set to link:link:spark-sql-catalyst-Optimizer.adoc#SparkOptimizer[SparkOptimizer] (that adds additional optimization batches). It is created for the session-owned <<catalog, catalog>>, link:spark-sql-SQLConf.adoc[SQLConf], and `ExperimentalMethods` (as defined in <<experimentalMethods, experimentalMethods>> attribute).

=== [[experimentalMethods]] `experimentalMethods`

`experimentalMethods` is...

=== [[sqlParser]] `sqlParser` Attribute

`sqlParser` is...

=== [[planner]] `planner` method

`planner` is the link:spark-sql-SparkPlanner.adoc[SparkPlanner] for the current session.

Whenever called, `planner` returns a new `SparkPlanner` instance with the link:spark-sparkcontext.adoc[SparkContext] of the current link:spark-sql-sparksession.adoc[SparkSession], the <<conf, SQLConf>>, and a collection of extra link:spark-sql-queryplanner.adoc#SparkStrategies[SparkStrategies] (via <<experimentalMethods, experimentalMethods>> attribute).

=== [[executePlan]] Preparing Logical Plan for Execution -- `executePlan` Method

[source, scala]
----
executePlan(plan: LogicalPlan): QueryExecution
----

`executePlan` executes the input link:spark-sql-LogicalPlan.adoc[LogicalPlan] to produce a link:spark-sql-query-execution.adoc[QueryExecution] in the current link:spark-sql-sparksession.adoc[SparkSession].

=== [[refreshTable]] `refreshTable` Method

`refreshTable` is...

=== [[addJar]] `addJar` Method

`addJar` is...

=== [[analyze]] `analyze` Method

`analyze` is...

=== [[streamingQueryManager]] `streamingQueryManager` Attribute

[source, scala]
----
streamingQueryManager: StreamingQueryManager
----

`streamingQueryManager` attribute points at shared link:spark-sql-streaming-StreamingQueryManager.adoc[StreamingQueryManager] (e.g. to link:spark-sql-streaming-DataStreamWriter.adoc#start[start streaming queries in `DataStreamWriter`]).

=== [[udf]] `udf` Attribute

[source, scala]
----
udf: UDFRegistration
----

`udf` attribute points at shared `UDFRegistration` for a given Spark session.

=== [[newHadoopConf]] Creating New Hadoop Configuration -- `newHadoopConf` Method

[source, scala]
----
newHadoopConf(): Configuration
----

`newHadoopConf` returns Hadoop's `Configuration` that it builds using link:spark-sparkcontext.adoc#hadoopConfiguration[SparkContext.hadoopConfiguration] (through link:spark-sql-sparksession.adoc[SparkSession]) with all configuration settings added.

NOTE: `newHadoopConf` is used by link:spark-sql-queryplanner.adoc#HiveSessionState[HiveSessionState] (for `HiveSessionCatalog`), `ScriptTransformation`, `ParquetRelation`, `StateStoreRDD`, and `SessionState` itself, and few other places.

CAUTION: FIXME What is `ScriptTransformation`? `StateStoreRDD`?
