== [[Optimizer]] Logical Query Plan Optimizer

`Optimizer` is a link:spark-sql-catalyst-analyzer.adoc#RuleExecutor[RuleExecutor] for link:spark-sql-LogicalPlan.adoc[logical plans]. It uses a <<batches, collection of logical plan optimizations>>.

NOTE: *Catalyst* is a Spark SQL framework for manipulating trees. It can work with trees of relational operators and expressions in link:spark-sql-LogicalPlan.adoc[logical plans] before they end up as link:spark-sql-SparkPlan.adoc[physical execution plans].

[source, scala]
----
scala> sql("select 1 + 1 + 1").explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias(((1 + 1) + 1), None)]
+- OneRowRelation$

== Analyzed Logical Plan ==
((1 + 1) + 1): int
Project [((1 + 1) + 1) AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Optimized Logical Plan ==
Project [3 AS ((1 + 1) + 1)#4]
+- OneRowRelation$

== Physical Plan ==
*Project [3 AS ((1 + 1) + 1)#4]
+- Scan OneRowRelation[]
----

Spark 2.0 uses Catalyst's tree manipulation library to build an extensible *query plan optimizer* with a number of query optimizations.

Catalyst supports both rule-based and cost-based optimization.

CAUTION: Review https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala[sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/Optimizer.scala].

=== [[batches]] Collection of Logical Plan Optimizations -- `batches` Method

[source, scala]
----
batches: Seq[Batch]
----

`batches` returns a collection of link:spark-sql-LogicalPlan.adoc[logical plan] optimization link:spark-sql-catalyst-analyzer.adoc#batch[batches].

.(A subset of) Catalyst Plan Optimizer's Logical Plan Optimizations
[cols="1,2",options="header",width="100%"]
|===
| Name | Description
| link:spark-sql-catalyst-constant-folding.adoc[Constant Folding] |
| link:spark-sql-catalyst-optimizer-PushDownPredicate.adoc[Predicate Pushdown] |
| link:spark-sql-catalyst-nullability-propagation.adoc[Nullability (NULL Value) Propagation] |
| link:spark-sql-catalyst-vectorized-parquet-decoder.adoc[Vectorized Parquet Decoder] |
| link:spark-sql-catalyst-optimizer-CombineTypedFilters.adoc[Combine Typed Filters] |
| link:spark-sql-catalyst-optimizer-PropagateEmptyRelation.adoc[Propagate Empty Relation] |
| link:spark-sql-catalyst-optimizer-SimplifyCasts.adoc[Simplify Casts] |
| link:spark-sql-catalyst-optimizer-ColumnPruning.adoc[Column Pruning] |
| link:spark-sql-catalyst-current-database-time.adoc[GetCurrentDatabase / ComputeCurrentTime] |
| link:spark-sql-catalyst-EliminateSerialization.adoc[Eliminate Serialization] |
|===

=== [[SparkOptimizer]] `SparkOptimizer` -- The Default Logical Query Plan Optimizer

`SparkOptimizer` is the default <<Optimizer, logical query plan optimizer>> that is available as link:spark-sql-sessionstate.adoc#optimizer[`optimizer` attribute of `SessionState`] with the <<batches, logical plan optimizations>>.

[source, scala]
----
sparkSession.sessionState.optimizer
----

NOTE: `SparkOptimizer` is _merely_ used to link:spark-sql-query-execution.adoc#optimizedPlan[compute the optimized `LogicalPlan` for a `QueryExecution`] (available as `optimizedPlan`).

`SparkOptimizer` requires a link:spark-sql-SessionCatalog.adoc[SessionCatalog], a link:spark-sql-SQLConf.adoc[SQLConf] and `ExperimentalMethods` with user-defined experimental methods.

NOTE: ``SparkOptimizer``'s input `experimentalMethods` serves an extension point for custom `ExperimentalMethods`.

`SparkOptimizer` extends the <<batches, `Optimizer` batches>> with the following batches:

1. *Optimize Metadata Only Query* (as `OptimizeMetadataOnlyQuery`)
2. *Extract Python UDF from Aggregate* (as `ExtractPythonUDFFromAggregate`)
3. *Prune File Source Table Partitions* (as `PruneFileSourcePartitions`)
4. *User Provided Optimizers* for the input user-defined `ExperimentalMethods`

You can see the result of executing `SparkOptimizer` on a query plan using link:spark-sql-query-execution.adoc#optimizedPlan[`optimizedPlan` attribute of `QueryExecution`].

[source, scala]
----
// Applying two filter in sequence on purpose
// We want to kick CombineTypedFilters optimizer in
val dataset = spark.range(10).filter(_ % 2 == 0).filter(_ == 0)

// optimizedPlan is a lazy value
// Only at the first time you call it you will trigger optimizations
// Next calls end up with the cached already-optimized result
// Use explain to trigger optimizations again
scala> dataset.queryExecution.optimizedPlan
res0: org.apache.spark.sql.catalyst.plans.logical.LogicalPlan =
TypedFilter <function1>, class java.lang.Long, [StructField(value,LongType,true)], newInstance(class java.lang.Long)
+- Range (0, 10, step=1, splits=Some(8))
----

[TIP]
====
Enable `DEBUG` or `TRACE` logging levels for `org.apache.spark.sql.execution.SparkOptimizer` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.sql.execution.SparkOptimizer=TRACE
```

Refer to link:spark-logging.adoc[Logging].
====

=== [[i-want-more]] Further reading or watching

1. https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html[Deep Dive into Spark SQLâ€™s Catalyst Optimizer]

2. (video) https://youtu.be/_1byVWTEK1s?t=19m7s[Modern Spark DataFrame and Dataset (Intermediate Tutorial)] by https://twitter.com/adbreind[Adam Breindel] from Databricks.
