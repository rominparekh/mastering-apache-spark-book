== ReceiverInputDStreams - Input Streams with Receivers

*Receiver Input Streams* (`ReceiverInputDStreams`) are specialized link:spark-streaming-inputdstreams.adoc[input streams] that use link:spark-streaming-receivers.adoc[receivers] to receive data (and hence the name which stands for an `InputDStream` with a receiver).

NOTE: Receiver input streams run receivers as long-running tasks that occupy a core per stream.

`ReceiverInputDStream` abstract class defines the following abstract method that custom implementations use to create receivers:

```
def getReceiver(): Receiver[T]
```

The receiver is then sent to and run on workers (when link:spark-streaming-receivertracker.adoc#starting[ReceiverTracker is started]).

[NOTE]
====
A fine example of a very minimalistic yet still useful implementation of `ReceiverInputDStream` class is the pluggable input stream `org.apache.spark.streaming.dstream.PluggableInputDStream` (https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/PluggableInputDStream.scala[the sources on GitHub]). It requires a `Receiver` to be given (by a developer) and simply returns it in `getReceiver`.

`PluggableInputDStream` is used by link:spark-streaming-streamingcontext.adoc#creating-receivers[StreamingContext.receiverStream()] method.
====

`ReceiverInputDStream` uses `ReceiverRateController` when link:spark-streaming-settings.adoc[spark.streaming.backpressure.enabled] is enabled.

[NOTE]
====
Both, `start()` and `stop` methods are implemented in `ReceiverInputDStream`, but do nothing. `ReceiverInputDStream` management is left to  link:spark-streaming-receivertracker.adoc[ReceiverTracker].

Read link:spark-streaming-receivertracker.adoc#ReceiverTrackerEndpoint-startReceiver[ReceiverTrackerEndpoint.startReceiver] for more details.
====

The source code of `ReceiverInputDStream` is https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/ReceiverInputDStream.scala[here at GitHub].

=== [[compute]] Generate RDDs for Batch Interval -- `compute` Method

The abstract `compute(validTime: Time): Option[RDD[T]]` method (from link:spark-streaming-dstreams.adoc[DStream]) uses link:spark-streaming-dstreamgraph.adoc[start time of  DStreamGraph], i.e. the start time of StreamingContext, to check whether `validTime` input parameter is really valid.

If the time to generate RDDs (`validTime`) is earlier than the start time of StreamingContext, an empty `BlockRDD` is generated.

Otherwise, link:spark-streaming-receivertracker.adoc[ReceiverTracker] is requested for all the blocks that have been allocated to this stream for this batch (using `ReceiverTracker.getBlocksOfBatch`).

The number of records received for the batch for the input stream (as link:spark-streaming-InputInfoTracker.adoc#StreamInputInfo[StreamInputInfo]) is registered with link:spark-streaming-InputInfoTracker.adoc#reportInfo[InputInfoTracker].

If all link:spark-blockdatamanager.adoc[BlockIds] have `WriteAheadLogRecordHandle`, a `WriteAheadLogBackedBlockRDD` is generated. Otherwise, a `BlockRDD` is.

=== [[back-pressure]] Back Pressure

CAUTION: FIXME

link:spark-streaming-backpressure.adoc[Back pressure] for input dstreams with receivers can be configured using link:spark-streaming-settings.adoc#back-pressure[spark.streaming.backpressure.enabled] setting.

NOTE: Back pressure is disabled by default.
