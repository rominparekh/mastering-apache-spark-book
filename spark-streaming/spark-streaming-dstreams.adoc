== [[DStream]] `DStream` -- Discretized Stream

*Discretized Stream (DStream)* is the fundamental concept of Spark Streaming. It is basically a stream of link:spark-rdd.adoc[RDDs] with elements being the data received from input streams for link:spark-streaming-streamingcontext.adoc[batch] (possibly extended in scope by link:spark-streaming-windowedoperators.adoc[windowed] or link:spark-streaming-operators-stateful.adoc[stateful] operators).

There is no notion of input and output dstreams. DStreams are all instances of `DStream` abstract class (see <<contract, DStream Contract>> in this document). You may however _correctly_ assume that all dstreams are input. And it happens to be so until you <<register, register a dstream>> that marks it as output.

.``DStream``'s Internal Properties
[cols="1,1,2",options="header",width="100%"]
|===
| Name | Initial Value | Description
| `storageLevel` | `NONE` | link:../spark-rdd-StorageLevel.adoc[StorageLevel] of the RDDs in the `DStream`.
| `restoredFromCheckpointData` | `false` | The flag to inform whether it was restored from checkpoint.
| `graph` | `null` | The reference to link:spark-streaming-dstreamgraph.adoc[DStreamGraph].
|===

A `DStream` is represented as https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/DStream.scala[org.apache.spark.streaming.dstream.DStream] abstract class.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.streaming.dstream.DStream` logger to see what happens inside a `DStream`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.dstream.DStream=DEBUG
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[contract]] DStream Contract

A `DStream` is defined by the following properties (with the names of the corresponding methods that subclasses have to implement):

* *dstream dependencies*, i.e. a collection of `DStreams` that this `DStream` depends on. They are often referred to as *parent dstreams*.
+
```
def dependencies: List[DStream[_]]
```

* *slide duration* (aka _slide interval_), i.e. a time interval after which the stream is requested to generate a RDD out of input data it consumes.
+
```
def slideDuration: Duration
```

* How to *compute* (_generate_) an optional RDD for the given batch if any. `validTime` is a point in time that marks the end boundary of slide duration.
+
```
def compute(validTime: Time): Option[RDD[T]]
```

=== [[creating-dstreams]] Creating DStreams

You can create dstreams through link:spark-streaming-streamingcontext.adoc#creating-receivers[the built-in input stream constructors using streaming context] or more specialized add-ons for external input data sources, e.g. link:spark-streaming-kafka.adoc[Apache Kafka].

NOTE: DStreams can only be created before link:spark-streaming-streamingcontext.adoc#start[`StreamingContext` is started].

=== [[zeroTime]] Zero Time (aka zeroTime)

*Zero time* (internally `zeroTime`) is the time when a <<initialize, dstream was initialized>>.

It serves as the initialization marker (via `isInitialized` method) and helps calculating intervals for RDD checkpointing (when <<internal-registries, checkpoint interval>> is set and the current batch time is a multiple thereof), link:spark-streaming-windowedoperators.adoc#slice[slicing], and the time validation for a batch (when <<getOrCompute, a dstream generates a RDD>>).

=== [[rememberDuration]][[remember-interval]] Remember Interval (aka rememberDuration)

*Remember interval* (internally `rememberDuration`) is the time interval for how long a dstream is supposed to remember (aka _cache_) RDDs created. This is a mandatory attribute of every dstream which is <<validateAtStart, validated at startup>>.

NOTE: It is used for <<clearMetadata, metadata cleanup>> of a dstream.

Initially, when a <<creating-dstreams, dstream is created>>, the remember interval is not set (i.e. `null`), but is set when the <<initialize, dstream is initialized>>.

It can be set to a custom value using <<remember, remember>> method.

NOTE: You may see the current value of remember interval when a dstream is <<validateAtStart, validated at startup>> and the log level is INFO.

=== [[generatedRDDs]] `generatedRDDs` - Internal Cache of Batch Times and Corresponding RDDs

`generatedRDDs` is an internal collection of pairs of batch times and the corresponding RDDs that were generated for the batch. It acts as a cache when <<getOrCompute, a dstream is requested to compute a RDD for batch>> (i.e. `generatedRDDs` may already have the RDD or gets a new RDD added).

`generatedRDDs` is empty initially, i.e. when a dstream is created.

It is a _transient_ data structure so it is not serialized when a dstream is. It is initialized to an empty collection when deserialized. You should see the following DEBUG message in the logs when it happens:

```
DEBUG [the simple class name of dstream].readObject used
```

As new RDDs are added, dstreams offer a way <<clearMetadata, to clear the old metadata>> during which the old RDDs are removed from `generatedRDDs` collection.

If link:spark-streaming-checkpointing.adoc[checkpointing is used], `generatedRDDs` collection can be link:spark-streaming-checkpointing.adoc#DStreamCheckpointData-restore[recreated from a storage].

=== [[initialize]] Initializing DStreams -- `initialize` Method

[source, scala]
----
initialize(time: Time): Unit
----

`initialize` method sets <<zeroTime, zero time>> and optionally <<checkpointDuration, checkpoint interval>> (if the dstream <<checkpointing, must checkpoint>> and the interval was not set already) and <<remember-interval, remember duration>>.

NOTE: `initialize` method is called for output dstreams only when link:spark-streaming-dstreamgraph.adoc#start[DStreamGraph is started].

The zero time of a dstream can only be set once or be set again to the same zero time. Otherwise, it throws `SparkException` as follows:

```
ZeroTime is already initialized to [zeroTime], cannot initialize it again to [time]
```

It verifies that <<checkpointing, checkpoint interval>> is defined when <<checkpointing, mustCheckpoint>> was enabled.

NOTE: The internal `mustCheckpoint` flag is disabled by default. It is set by custom dstreams like link:spark-streaming-statedstreams.adoc[StateDStreams].

If `mustCheckpoint` is enabled and the checkpoint interval was not set, it is automatically set to the <<contract, slide interval>> or 10 seconds, whichever is longer. You should see the following INFO message in the logs when the checkpoint interval was set automatically:

```
INFO [DStreamType]: Checkpoint interval automatically set to [checkpointDuration]
```

It then ensures that <<rememberDuration, remember interval>> is at least twice the checkpoint interval (only if defined) or the slide duration.

At the very end, it initializes the parent dstreams (available as <<contract, dependencies>>) that recursively initializes the entire graph of dstreams.

=== [[remember]] `remember` Method

[source, scala]
----
remember(duration: Duration): Unit
----

`remember` sets <<rememberDuration, remember interval>> for the current dstream and the dstreams it depends on (see <<contract, dependencies>>).

If the input `duration` is specified (i.e. not `null`), `remember` allows setting the remember interval (only when the current value was not set already) or extend it (when the current value is shorter).

You should see the following INFO message in the logs when the remember interval changes:

```
INFO Duration for remembering RDDs set to [rememberDuration] for [dstream]
```

At the end, `remember` always sets the current <<rememberDuration, remember interval>> (whether it was set, extended or did not change).

=== [[checkpoint]] Checkpointing DStreams -- `checkpoint` Method

[source, scala]
----
checkpoint(interval: Duration): DStream[T]
----

You use `checkpoint(interval: Duration)` method to set up a periodic checkpointing every (checkpoint) `interval`.

You can only enable checkpointing and set the checkpoint interval before link:spark-streaming-streamingcontext.adoc#start[StreamingContext is started] or `UnsupportedOperationException` is thrown as follows:

```
java.lang.UnsupportedOperationException: Cannot change checkpoint interval of an DStream after streaming context has started
  at org.apache.spark.streaming.dstream.DStream.checkpoint(DStream.scala:177)
  ... 43 elided
```

Internally, `checkpoint` method calls link:spark-streaming-dstreams.adoc#cache-persist[persist] (that sets the default `MEMORY_ONLY_SER` storage level).

If checkpoint interval is set, the link:spark-streaming-streamingcontext.adoc#checkpoint-directory[checkpoint directory] is mandatory. Spark validates it when link:spark-streaming-streamingcontext.adoc#start[StreamingContext starts] and throws a `IllegalArgumentException` exception if not set.

```
java.lang.IllegalArgumentException: requirement failed: The checkpoint directory has not been set. Please set it by StreamingContext.checkpoint().
```

You can see the value of the checkpoint interval for a dstream in the logs when  link:spark-streaming-dstreams.adoc#validateAtStart[it is validated]:

```
INFO Checkpoint interval = [checkpointDuration]
```

=== [[checkpointing]] Checkpointing

DStreams can link:spark-streaming-checkpointing.adoc[checkpoint] input data at specified time intervals.

The following settings are internal to a dstream and define how it checkpoints the input data if any.

* `mustCheckpoint` (default: `false`) is an internal private flag that marks a dstream as being checkpointed (`true`) or not (`false`). It is an implementation detail and the author of a `DStream` implementation sets it.
+
Refer to <<initialize, Initializing DStreams (initialize method)>> to learn how it is used to set the checkpoint interval, i.e. `checkpointDuration`.

* `checkpointDuration` is a configurable property that says how often a dstream checkpoints data. It is often called *checkpoint interval*. If not set explicitly, but the dstream is checkpointed, it will be while <<initialize, initializing dstreams>>.

* `checkpointData` is an instance of link:spark-streaming-checkpointing.adoc#checkpoint-data[DStreamCheckpointData].

* `restoredFromCheckpointData` (default: `false`) is an internal flag to describe the initial state of a dstream, i.e.. whether (`true`) or not (`false`) it was started by restoring state from checkpoint.

=== [[validateAtStart]] Validating Setup at Startup -- `validateAtStart` Method

CAUTION: FIXME Describe me!

=== [[register]] Registering Output Streams -- `register` Method

[source, scala]
----
register(): DStream[T]
----

`DStream` by design has no notion of being an output stream. It is link:spark-streaming-dstreamgraph.adoc[DStreamGraph] to know and be able to differentiate between input and output streams.

`DStream` comes with internal `register` method that registers a `DStream` as an output stream.

The internal private `foreachRDD` method uses `register` to register output streams to link:spark-streaming-dstreamgraph.adoc[DStreamGraph]. Whenever called, it creates link:spark-streaming-foreachdstreams.adoc[ForEachDStream] and calls `register` upon it. That is how streams become output streams.

=== [[generateJob]] Generating Streaming Job For Batch For Output DStream -- `generateJob` Internal Method

[source, scala]
----
generateJob(time: Time): Option[Job]
----

`generateJob` generates a streaming job for a `time` batch for a (output) dstream. It may or may not generate a streaming job for the requested batch `time` if <<getOrCompute, there are RDDs to process>>.

NOTE: `generateJob` is called when link:spark-streaming-dstreamgraph.adoc#generateJobs[`DStreamGraph` generates jobs for a batch time].

It <<getOrCompute, computes an RDD for the batch>> and, if there is one, returns a link:spark-streaming.adoc#Job[streaming job] for the batch `time` and a job function that will link:../spark-sparkcontext.adoc#runJob[run a Spark job] (with the generated RDD and the job function) when executed.

NOTE: The Spark job uses an empty function to calculate partitions of a RDD.

CAUTION: FIXME What happens when `SparkContext.runJob(rdd, emptyFunc)` is called with the empty function, i.e. `(iterator: Iterator[T]) => {}`?

=== [[getOrCompute]] Computing RDD for Batch -- `getOrCompute` Internal Method

[source, scala]
----
getOrCompute(time: Time): Option[RDD[T]]
----

`getOrCompute` returns an optional `RDD` for a `time` batch.

NOTE: `getOrCompute` is `private[streaming] final` method.

`getOrCompute` uses <<generatedRDDs, generatedRDDs>> to return the RDD if it has already been generated for the `time`. If not, it generates one by <<contract, computing the input stream>> (using `compute(validTime: Time)` method).

If there was anything to process in the input stream, i.e. <<contract, computing the input stream returned a RDD>>, the RDD is first link:spark-rdd-caching.adoc[persisted] (only if `storageLevel` for the input stream is different from link:../spark-rdd-StorageLevel.adoc[`NONE` storage level]).

You should see the following DEBUG message in the logs:

```
DEBUG Persisting RDD [id] for time [time] to [storageLevel]
```

The generated RDD is link:spark-rdd-checkpointing.adoc[checkpointed] if <<internal-registries, checkpointDuration>> is defined and the time interval between current and <<internal-registries, zero>> times is a multiple of <<internal-registries, checkpointDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Marking RDD [id] for time [time] for checkpointing
```

The generated RDD is saved in the <<internal-registries, internal generatedRDDs registry>>.

NOTE: `getOrCompute` is used when a `DStream` is requested to <<generateJob, generate a streaming job for a batch>>.

=== [[cache-persist]] Caching and Persisting

CAUTION: FIXME

=== [[clearCheckpointData]] Checkpoint Cleanup

CAUTION: FIXME

=== [[restoreCheckpointData]] `restoreCheckpointData`

[source, scala]
----
restoreCheckpointData(): Unit
----

`restoreCheckpointData` does its work only when the internal _transient_ `restoredFromCheckpointData` flag is disabled (i.e. `false`) and is so initially.

NOTE: `restoreCheckpointData` method is called when link:spark-streaming-dstreamgraph.adoc#restoreCheckpointData[DStreamGraph is requested to restore state of output dstreams].

If `restoredFromCheckpointData` is disabled, you should see the following INFO message in the logs:

```
INFO ...DStream: Restoring checkpoint data
```

link:spark-streaming-checkpointing.adoc#DStreamCheckpointData-restore[DStreamCheckpointData.restore()] is executed. And then `restoreCheckpointData` method is executed for every dstream the current dstream depends on (see <<contract, DStream Contract>>).

Once completed, the internal `restoredFromCheckpointData` flag is enabled (i.e. `true`) and you should see the following INFO message in the logs:

```
INFO Restored checkpoint data
```

=== [[clearMetadata]] Metadata Cleanup -- `clearMetadata` Method

NOTE: It is called when  link:spark-streaming-dstreamgraph.adoc#clearMetadata[DStreamGraph clears metadata for every output stream].

`clearMetadata(time: Time)` is called to remove old RDDs that have been generated so far (and collected in <<generatedRDDs, generatedRDDs>>). It is a sort of _garbage collector_.

When `clearMetadata(time: Time)` is called, it checks link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag (default enabled).

It collects generated RDDs (from <<generatedRDDs, generatedRDDs>>) that are older than <<internal-registries, rememberDuration>>.

You should see the following DEBUG message in the logs:

```
DEBUG Clearing references to old RDDs: [[time] -> [rddId], ...]
```

Regardless of link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag, all the collected RDDs are removed from <<generatedRDDs, generatedRDDs>>.

When link:spark-streaming-settings.adoc[spark.streaming.unpersist] flag is set (it is by default), you should see the following DEBUG message in the logs:

```
DEBUG Unpersisting old RDDs: [id1, id2, ...]
```

For every RDD in the list, it link:spark-rdd-caching.adoc#unpersist[unpersists them (without blocking)] one by one and explicitly link:spark-rdd-blockrdd.adoc[removes blocks for BlockRDDs]. You should see the following INFO message in the logs:

```
INFO Removing blocks of RDD [blockRDD] of time [time]
```

After RDDs have been removed from <<generatedRDDs, generatedRDDs>> (and perhaps unpersisted), you should see the following DEBUG message in the logs:

```
DEBUG Cleared [size] RDDs that were older than [time]: [time1, time2, ...]
```

The stream passes the call to clear metadata to its <<contract, dependencies>>.

=== [[updateCheckpointData]] `updateCheckpointData` Method

[source, scala]
----
updateCheckpointData(currentTime: Time): Unit
----

NOTE: It is called when link:spark-streaming-dstreamgraph.adoc#updateCheckpointData[DStreamGraph is requested to do updateCheckpointData itself].

When `updateCheckpointData` is called, you should see the following DEBUG message in the logs:

```
DEBUG Updating checkpoint data for time [currentTime] ms
```

It then executes link:spark-streaming-checkpointing.adoc#DStreamCheckpointData-update[DStreamCheckpointData.update(currentTime)] and calls `updateCheckpointData` method on each dstream the dstream depends on.

When `updateCheckpointData` finishes, you should see the following DEBUG message in the logs:

```
DEBUG Updated checkpoint data for time [currentTime]: [checkpointData]
```
