== [[KafkaRDD]] KafkaRDD

`KafkaRDD` is a link:../spark-rdd.adoc[RDD] of Kafka's `ConsumerRecords` from topics in Apache Kafka with support for link:spark-streaming-kafka-HasOffsetRanges.adoc[HasOffsetRanges].

NOTE: Kafka's https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html[ConsumerRecord] holds a topic name, a partition number, the offset of the record in the Kafka partition and the record itself (as a key-value pair).

`KafkaRDD` uses <<KafkaRDDPartition, KafkaRDDPartition>> as the partitions that <<getPreferredLocations, define their preferred locations>> (as the host of the topic).

NOTE: The feature of defining placement preference (aka _location preference_) very well maps a `KafkaRDD` partition to a Kafka topic partition on a Kafka-closest host.

`KafkaRDD` is created:

* On demand using link:spark-streaming-kafka-KafkaUtils.adoc#createRDD[KafkaUtils.createRDD]

* In batches using link:spark-streaming-kafka-KafkaUtils.adoc#createDirectStream[KafkaUtils.createDirectStream]

`KafkaRDD` is also created when a `DirectKafkaInputDStream` restores `KafkaRDDs` from checkpoint.

NOTE: `KafkaRDD` is a `private[spark]` class.

[TIP]
====
Enable `INFO` logging level for `org.apache.spark.streaming.kafka010.KafkaRDD` logger to see what happens inside.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.kafka010.KafkaRDD=INFO
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[getPartitions]] `getPartitions` Method

CAUTION: FIXME

=== [[creating-instance]] Creating KafkaRDD Instance

`KafkaRDD` takes the following when created:

* [[sc]] link:../spark-sparkcontext.adoc[SparkContext]
* [[kafkaParams]] Collection of Kafka parameters with their values
* [[offsetRanges]] Collection of link:spark-streaming-kafka-HasOffsetRanges.adoc#OffsetRange[OffsetRanges]
* [[preferredHosts]] Kafka's `TopicPartitions` and their hosts
* [[useConsumerCache]] Flag to control whether to use consumer cache

CAUTION: FIXME Are the hosts in `preferredHosts` Kafka brokers?

`KafkaRDD` initializes the <<internal-registries, internal registries and counters>>.

=== [[compute]] Computing KafkaRDDPartition (in TaskContext) -- `compute` Method

[source, scala]
----
compute(thePart: Partition, context: TaskContext): Iterator[ConsumerRecord[K, V]]
----

NOTE: `compute` is a part of the link:../spark-rdd.adoc#compute[RDD Contract].

`compute` assumes that it works with `thePart` as <<KafkaRDDPartition, KafkaRDDPartition>> only. It asserts that the offsets are correct, i.e. `fromOffset` is at most `untilOffset`.

If the beginning and ending offsets are the same, you should see the following INFO message in the logs and `compute` returns an empty collection.

```
INFO KafkaRDD: Beginning offset [fromOffset] is the same as ending offset skipping [topic] [partition]
```

Otherwise, when the beginning and ending offsets are different, a <<KafkaRDDIterator, KafkaRDDIterator>> is created (for the partition and the input link:../spark-taskscheduler-taskcontext.adoc[TaskContext]) and returned.

=== [[getPreferredLocations]] Getting Placement Preferences of Partition -- `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(thePart: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of link:../spark-rdd.adoc#getPreferredLocations[RDD contract] to define the placement preferences (aka _preferred locations_) of a partition.

`getPreferredLocations` casts `thePart` to <<KafkaRDDPartition, KafkaRDDPartition>>.

`getPreferredLocations` <<executors, finds all executors>>.

CAUTION: FIXME Use proper name for executors.

`getPreferredLocations` <<topicPartition, requests `KafkaRDDPartition` for the Kafka `TopicPartition`>> and finds the <<preferredHosts, preferred hosts>> for the partition.

NOTE: `getPreferredLocations` uses <<preferredHosts, preferredHosts>> that was given when <<creating-instance, `KafkaRDD` was created>>.

If `getPreferredLocations` did not find the preferred host for the partition, all executors are used. Otherwise, `getPreferredLocations` includes only executors on the preferred host.

If `getPreferredLocations` found no executors, all the executors are considered.

`getPreferredLocations` returns one matching executor (for the `TopicPartition`) or an empty collection.

=== [[executors]] Creating ExecutorCacheTaskLocations for All Executors in Cluster -- `executors` Internal Method

[source, scala]
----
executors(): Array[ExecutorCacheTaskLocation]
----

`executors` link:spark-BlockManagerMaster.adoc#getPeers[requests `BlockManagerMaster` for all `BlockManager` nodes (peers) in a cluster] (that represent all the executors available).

NOTE: `executors` uses ``KafkaRDD``'s link:spark-sparkcontext.adoc[SparkContext] to link:spark-blockmanager.adoc#blockManager[access the current `BlockManager`] and in turn link:spark-blockmanager.adoc#master[BlockManagerMaster].

`executors` creates `ExecutorCacheTaskLocations` using the peers' hosts and executor ids.

NOTE: `executors` are sorted by their host names and executor ids.

CAUTION: FIXME Image for sorted ExecutorCacheTaskLocations.

NOTE: `executors` is used exclusively when <<getPreferredLocations, `KafkaRDD` is requested for its placement preferences>> (aka _preferred locations_).

=== [[KafkaRDDPartition]] `KafkaRDDPartition`

`KafkaRDDPartition` is...FIXME

==== [[topicPartition]] `topicPartition`

CAUTION: FIXME
