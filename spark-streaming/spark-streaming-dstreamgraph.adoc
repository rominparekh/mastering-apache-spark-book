== DStreamGraph

`DStreamGraph` (is a final helper class that) manages *input* and *output dstreams*. It also holds <<zeroTime, zero time>> for the other components that marks the time when <<start, it was started>>.

`DStreamGraph` maintains the collections of link:spark-streaming-inputdstreams.adoc[InputDStream] instances (as `inputStreams`) and output link:spark-streaming-dstreams.adoc[DStream] instances (as `outputStreams`), but, more importantly, <<generateJobs, it generates streaming jobs for output streams for a batch (time)>>.

`DStreamGraph` holds the <<batchDuration, batch interval>> for the other parts of a Streaming application.

[TIP]
====
Enable `INFO` or `DEBUG` logging level for `org.apache.spark.streaming.DStreamGraph` logger to see what happens in `DStreamGraph`.

Add the following line to `conf/log4j.properties`:

```
log4j.logger.org.apache.spark.streaming.DStreamGraph=DEBUG
```

Refer to link:../spark-logging.adoc[Logging].
====

=== [[zero-time]][[zeroTime]] Zero Time (aka zeroTime)

*Zero time* (internally `zeroTime`) is the time when <<start, DStreamGraph has been started>>.

It is passed on down the output dstream graph so link:spark-streaming-dstreams.adoc#initialize[output dstreams can initialize themselves].

=== [[startTime]] Start Time (aka startTime)

*Start time* (internally `startTime`) is the time when <<start, DStreamGraph has been started>> or <<restart, restarted>>.

NOTE: At regular start start time is exactly <<zeroTime, zero time>>.

=== [[batchDuration]][[batch-interval]] Batch Interval (aka batchDuration)

`DStreamGraph` holds the *batch interval* (as `batchDuration`) for the other parts of a Streaming application.

`setBatchDuration(duration: Duration)` is the method to set the batch interval.

It appears that it is _the_ place for the value since it must be set before link:spark-streaming-jobgenerator.adoc[JobGenerator] can be instantiated.

It _is_ set while link:spark-streaming-streamingcontext.adoc[StreamingContext] is being instantiated and is validated (using `validate()` method of `StreamingContext` and `DStreamGraph`) before `StreamingContext` is started.

=== [[getMaxInputStreamRememberDuration]][[maximum-remember-interval]] Maximum Remember Interval -- getMaxInputStreamRememberDuration Method

[source, scala]
----
getMaxInputStreamRememberDuration(): Duration
----

*Maximum Remember Interval* is the maximum link:spark-streaming-dstreams.adoc#remember-interval[remember interval] across all the input dstreams. It is calculated using `getMaxInputStreamRememberDuration` method.

NOTE: It is called when JobGenerator is requested to link:spark-streaming-jobgenerator.adoc#clearMetadata[clear metadata] and link:spark-streaming-jobgenerator.adoc#clearCheckpointData[checkpoint data].

=== [[input-dstream-registry]] Input DStreams Registry

CAUTION: FIXME

=== [[output-dstreams]][[output-dstream-registry]] Output DStreams Registry

`DStream` by design has no notion of being an output dstream. To mark a dstream as output you need to link:spark-streaming-dstreams.adoc#register[register a dstream (using DStream.register method)] which happens for...FIXME

=== [[start]] Starting `DStreamGraph`

[source, scala]
----
start(time: Time): Unit
----

When `DStreamGraph` is started (using `start` method), it sets <<zeroTime, zero time>> and <<startTime, start time>>.

NOTE: `start` method is called when link:spark-streaming-jobgenerator.adoc#starting[JobGenerator starts for the first time] (not from a checkpoint).

NOTE: You can start `DStreamGraph` as many times until `time` is not `null` and <<zeroTime, zero time>> has been set.

(_output dstreams_) `start` then walks over the collection of output dstreams and for each output dstream, one at a time, calls their link:spark-streaming-dstreams.adoc#initialize[initialize(zeroTime)], link:spark-streaming-dstreams.adoc#remember[remember] (with the current <<rememberDuration, remember interval>>), and link:spark-streaming-dstreams.adoc#validateAtStart[validateAtStart] methods.

(_input dstreams_) When all the output streams are processed, it starts the input dstreams (in parallel) using `start` method.

=== [[stop]] Stopping `DStreamGraph`

[source, scala]
----
stop(): Unit
----

CAUTION: FIXME

=== [[restart]] Restarting `DStreamGraph`

[source, scala]
----
restart(time: Time): Unit
----

`restart` sets <<startTime, start time>> to be `time` input parameter.

NOTE: This is the only moment when <<zeroTime, zero time>> can be different than <<startTime, start time>>.

CAUTION: `restart` doesn't seem to be called ever.

=== [[generateJobs]] Generating Streaming Jobs for Output DStreams for Batch Time -- `generateJobs` Method

[source, scala]
----
generateJobs(time: Time): Seq[Job]
----

`generateJobs` method generates a collection of streaming jobs for output streams for a given batch `time`. It walks over each link:spark-streaming-dstreams.adoc#register[registered output stream] (in `outputStreams` internal registry) and link:spark-streaming-dstreams.adoc#generateJob[requests each stream for a streaming job]

NOTE: `generateJobs` is called by link:spark-streaming-jobgenerator.adoc[JobGenerator] to link:spark-streaming-jobgenerator.adoc#generateJobs[generate jobs for a given batch time] or link:spark-streaming-jobgenerator.adoc#restarting[when restarted from checkpoint].

When `generateJobs` method executes, you should see the following DEBUG message in the logs:

```
DEBUG DStreamGraph: Generating jobs for time [time] ms
```

`generateJobs` then walks over each link:spark-streaming-dstreams.adoc#register[registered output stream] (in `outputStreams` internal registry) and link:spark-streaming-dstreams.adoc#generateJob[requests the streams for a streaming job].

Right before the method finishes, you should see the following DEBUG message with the number of streaming jobs generated (as `jobs.length`):

```
DEBUG DStreamGraph: Generated [jobs.length] jobs for time [time] ms
```

=== [[validate]][[dstreamgraph-validation]] Validation Check

`validate()` method checks whether batch duration and at least one output stream have been set. It will throw `java.lang.IllegalArgumentException` when either is not.

NOTE: It is called when link:spark-streaming-streamingcontext.adoc#start[StreamingContext starts].

=== [[clearMetadata]] Metadata Cleanup

NOTE: It is called when  link:spark-streaming-jobgenerator.adoc#ClearMetadata[JobGenerator clears metadata].

When `clearMetadata(time: Time)` is called, you should see the following DEBUG message in the logs:

```
DEBUG DStreamGraph: Clearing metadata for time [time] ms
```

It merely walks over the collection of output streams and (synchronously, one by one) asks to do link:spark-streaming-dstreams.adoc#clearMetadata[its own metadata cleaning].

When finishes, you should see the following DEBUG message in the logs:

```
DEBUG DStreamGraph: Cleared old metadata for time [time] ms
```

=== [[restoreCheckpointData]] Restoring State for Output DStreams -- `restoreCheckpointData` Method

[source, scala]
----
restoreCheckpointData(): Unit
----

When `restoreCheckpointData()` is executed, you should see the following INFO message in the logs:

```
INFO DStreamGraph: Restoring checkpoint data
```

Then, every <<output-dstreams, output dstream>> is requested to link:spark-streaming-dstreams.adoc#restoreCheckpointData[restoreCheckpointData].

At the end, you should see the following INFO message in the logs:

```
INFO DStreamGraph: Restored checkpoint data
```

NOTE: `restoreCheckpointData` is executed when link:spark-streaming-checkpointing.adoc#recreating-streamingcontext[StreamingContext is recreated from checkpoint].

=== [[updateCheckpointData]] Updating Checkpoint Data -- `updateCheckpointData` Method

[source, scala]
----
updateCheckpointData(time: Time): Unit
----

NOTE: `updateCheckpointData` is called when link:spark-streaming-jobgenerator.adoc#DoCheckpoint[JobGenerator processes DoCheckpoint events].

When `updateCheckpointData` is called, you should see the following INFO message in the logs:

```
INFO DStreamGraph: Updating checkpoint data for time [time] ms
```

It then walks over every output dstream and calls its link:spark-streaming-dstreams.adoc#updateCheckpointData[updateCheckpointData(time)].

When `updateCheckpointData` finishes it prints out the following INFO message to the logs:

```
INFO DStreamGraph: Updated checkpoint data for time [time] ms
```

=== [[clearCheckpointData]] Checkpoint Cleanup -- `clearCheckpointData` Method

[source, scala]
----
clearCheckpointData(time: Time)
----

NOTE: `clearCheckpointData` is called when  link:spark-streaming-jobgenerator.adoc#clearCheckpointData[JobGenerator clears checkpoint data].

When `clearCheckpointData` is called, you should see the following INFO message in the logs:

```
INFO DStreamGraph: Clearing checkpoint data for time [time] ms
```

It merely walks through the collection of output streams and (synchronously, one by one) asks to do link:spark-streaming-dstreams.adoc#clearCheckpointData[their own checkpoint data cleaning].

When finished, you should see the following INFO message in the logs:

```
INFO DStreamGraph: Cleared checkpoint data for time [time] ms
```

=== [[rememberDuration]][[remember-interval]] Remember Interval

*Remember interval* is the time to remember (aka _cache_) the RDDs that have been generated by (output) dstreams in the context (before they are released and garbage collected).

It can be set using <<remember, remember>> method.

=== [[remember]] `remember` Method

[source, scala]
----
remember(duration: Duration): Unit
----

`remember` method simply sets <<rememberDuration, remember  interval>> and exits.

NOTE: It is called by link:spark-streaming-streamingcontext.adoc#remember[StreamingContext.remember] method.

It first checks whether or not it has been set already and if so, throws `java.lang.IllegalArgumentException` as follows:

[options="wrap"]
----
java.lang.IllegalArgumentException: requirement failed: Remember duration already set as [rememberDuration] ms. Cannot set it again.
  at scala.Predef$.require(Predef.scala:219)
  at org.apache.spark.streaming.DStreamGraph.remember(DStreamGraph.scala:79)
  at org.apache.spark.streaming.StreamingContext.remember(StreamingContext.scala:222)
  ... 43 elided
----

NOTE: It only makes sense to call `remember` method before <<start, DStreamGraph is started>>, i.e. before link:spark-streaming-streamingcontext.adoc#start[StreamingContext is started], since the output dstreams are only given the remember interval when DStreamGraph starts.
