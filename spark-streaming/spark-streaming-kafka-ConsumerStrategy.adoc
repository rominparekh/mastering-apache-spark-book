== [[ConsumerStrategy]] `ConsumerStrategy` -- Kafka Consumers' Post-Configuration API

`ConsumerStrategy` is a <<contract, contract>> to create Kafka Consumers in a Spark Streaming application that allows for their custom configuration after the consumers have been created.

NOTE: Kafka consumers read records from topic partitions in a Kafka cluster.

`ConsumerStrategy[K, V]` is an abstract class with two methods, i.e. <<executorKafkaParams, executorKafkaParams>> and <<onStart, onStart>>.

.ConsumerStrategy Contract and DirectKafkaInputDStream
[cols="1,2",options="header",width="100%"]
|===
| Consumer Strategy | DirectKafkaInputDStream Usage
| <<executorKafkaParams, executorKafkaParams>> | Used when a link:spark-streaming-kafka-DirectKafkaInputDStream.adoc#creating-instance[`DirectKafkaInputDStream` is created] to initialize internal state.
| <<onStart, onStart>> | Used to link:spark-streaming-kafka-DirectKafkaInputDStream.adoc#consumer[create a Kafka consumer (in `DirectKafkaInputDStream`)]
|===

The following table are the Kafka Consumer strategies currently available in Spark 2.0.

.Kafka Consumer Strategies in Spark Streaming
[cols="1,2",options="header",width="100%"]
|===
| Consumer Strategy | Description
| <<Assign, Assign>> |
| <<Subscribe, Subscribe>> |
| <<SubscribePattern, SubscribePattern>> |
|===

You can access the predefined `ConsumerStrategy` implementations using link:spark-streaming-kafka-ConsumerStrategies.adoc[ConsumerStrategies factory object].

[source, scala]
----
import org.apache.spark.streaming.kafka010.ConsumerStrategies

val topics = List("topic1")
import org.apache.kafka.common.serialization.StringDeserializer
val kafkaParams = Map(
  "bootstrap.servers" -> "localhost:9092",
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "spark-streaming-notes",
  "auto.offset.reset" -> "earliest"
)
import org.apache.kafka.common.TopicPartition
val offsets = Map(new TopicPartition("topic3", 0) -> 2L)

val subscribeStrategy = ConsumerStrategies.Subscribe[String, String](topics, kafkaParams, offsets)
----

=== [[contract]] ConsumerStrategy Contract

==== [[executorKafkaParams]] `executorKafkaParams` Method

[source, scala]
----
executorKafkaParams: ju.Map[String, Object]
----

==== [[onStart]] `onStart` Method

[source, scala]
----
onStart(currentOffsets: ju.Map[TopicPartition, jl.Long]): Consumer[K, V]
----

=== [[Assign]] Assign Strategy

[source, scala]
----
class Assign[K, V](
  topicPartitions: java.util.Collection[TopicPartition],
  kafkaParams: java.util.Map[String, Object],
  offsets: java.util.Map[TopicPartition, java.util.Long]
) extends ConsumerStrategy[K, V]
----

`Assign` returns the input `kafkaParams` directly from <<executorKafkaParams, executorKafkaParams>> method.

For `onStart`, `Assign` creates a `KafkaConsumer` (with `kafkaParams`) and explicitly assigns the list of partitions `topicPartitions` to this consumer (using Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#assign(java.util.Collection)++[KafkaConsumer.assign] method). It then overrides the fetch offsets that the consumer will use (on the next link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll(long)++[poll]) to ``onStart``'s input `currentOffsets` or `offsets` whatever is not empty (using Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#seek(org.apache.kafka.common.TopicPartition,%20long)++[KafkaConsumer.seek] method).

=== [[Subscribe]] Subscribe Strategy

[source, scala]
----
class Subscribe[K, V](
  topics: java.util.Collection[jl.String],
  kafkaParams: java.util.Map[String, Object],
  offsets: java.util.Map[TopicPartition, java.util.Long]
) extends ConsumerStrategy[K, V]
----

`Subscribe` returns the input `kafkaParams` directly from <<executorKafkaParams, executorKafkaParams>> method.

For `onStart`, `Subscribe` creates a `KafkaConsumer` (with `kafkaParams`) and subscribes to `topics` (using Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe] method). For non-empty `currentOffsets` or `offsets` (whatever is not empty in that order), `onStart` polls data for topics or partitions (using Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll(long)++[KafkaConsumer.poll] method). It then overrides the fetch offsets that the consumer will use (on the next link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#poll(long)++[poll]) to ``onStart``'s input `currentOffsets` or `offsets` whatever is not empty (using Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#seek(org.apache.kafka.common.TopicPartition,%20long)++[KafkaConsumer.seek] method).

TIP: You can suppress Kafka's `NoOffsetForPartitionException` with Kafka's `auto.offset.reset` setting set to `NONE` in `kafkaParams`.

In case of Kafka's `NoOffsetForPartitionException` with exception suppression enabled, you can see the following WARN message in the logs:

```
WARN Catching NoOffsetForPartitionException since auto.offset.reset is none.  See KAFKA-3370
```

TIP: Read through https://issues.apache.org/jira/browse/KAFKA-3370[KAFKA-3370: Add options to auto.offset.reset to reset offsets upon initialization only]

[source, scala]
----
??? FIXME Example with the WARN above
----

=== [[SubscribePattern]] SubscribePattern Strategy

[source, scala]
----
class SubscribePattern[K, V](
    pattern: java.util.regex.Pattern,
    kafkaParams: java.util.Map[String, Object],
    offsets: java.util.Map[TopicPartition, java.util.Long]
) extends ConsumerStrategy[K, V]
----

`SubscribePattern` returns the input `kafkaParams` directly from <<executorKafkaParams, executorKafkaParams>> method.

For `onStart`, `SubscribePattern` creates a `KafkaConsumer` (with `kafkaParams`) and subscribes to `pattern` topics with Kafka's internal `NoOpConsumerRebalanceListener` (using Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe] method).

NOTE: The only difference between <<SubscribePattern, SubscribePattern>> and <<Subscribe, Subscribe>> Consumer strategies is the use of Kafka's link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection,%20org.apache.kafka.clients.consumer.ConsumerRebalanceListener)++[KafkaConsumer.subscribe(Collection, ConsumerRebalanceListener)] and link:++https://kafka.apache.org/0100/javadoc/org/apache/kafka/clients/consumer/KafkaConsumer.html#subscribe(java.util.Collection)++[KafkaConsumer.subscribe(Collection)] methods, respectively.
