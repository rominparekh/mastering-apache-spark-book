== [[BaseRelation]] BaseRelation

`BaseRelation` works in a link:spark-sql-sqlcontext.adoc[SQLContext] with a data of a given schema (as link:spark-sql-StructType.adoc[StructType]). `BaseRelation` knows its size (as `sizeInBytes`), whether it needs a conversion, and computes the list of `Filter` that this data source may not be able to handle.

.BaseRelation Methods
[cols="1,3",options="header",width="100%"]
|===
| Name | Behaviour
| `sqlContext` | Returns the current link:spark-sql-sqlcontext.adoc[SQLContext].

| `schema` | Returns the current link:spark-sql-StructType.adoc[StructType].

| `sizeInBytes` | Computes an estimated size of this relation in bytes.

| `needConversion` | Whether the relation needs a conversion of the objects in `Row` to internal representation.

| `unhandledFilters` | Computes the list of ``Filter``s that this data source may not be able to handle.
|===

NOTE: A "data source" and "relation" appear as synonyms.

`BaseRelation` is an abstract class in `org.apache.spark.sql.sources` package.

=== [[HadoopFsRelation]] HadoopFsRelation

[source, scala]
----
case class HadoopFsRelation(
  location: FileIndex,
  partitionSchema: StructType,
  dataSchema: StructType,
  bucketSpec: Option[BucketSpec],
  fileFormat: FileFormat,
  options: Map[String, String])(val sparkSession: SparkSession)
extends BaseRelation with FileRelation
----

`HadoopFsRelation` is a <<BaseRelation, BaseRelation>> in a link:spark-sql-sparksession.adoc[SparkSession] (through which it gets to the current link:spark-sql-sqlcontext.adoc[SQLContext]).

`HadoopFsRelation` requires a schema (as link:spark-sql-StructType.adoc[StructType]) that it expands with the input `partitionSchema` schema.

`sizeInBytes` and `inputFiles` (from the base `BaseRelation`) use the input `FileIndex` to compute the size and input files, respectively.
