== [[ShuffledRowRDD]] ShuffledRowRDD

`ShuffledRowRDD` is a specialized link:spark-rdd.adoc[RDD] of link:spark-sql-InternalRow.adoc[InternalRow]s.

NOTE: `ShuffledRowRDD` looks like link:spark-rdd-ShuffledRDD.adoc[ShuffledRDD], and the difference is in the type of the values to process, i.e. link:spark-sql-InternalRow.adoc[InternalRow] and `(K, C)` key-value pairs, respectively.

`ShuffledRowRDD` takes a link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] (of integer keys and link:spark-sql-InternalRow.adoc[InternalRow] values).

NOTE: The `dependency` property is mutable and is of type `ShuffleDependency[Int, InternalRow, InternalRow]`.

`ShuffledRowRDD` takes an optional `specifiedPartitionStartIndices` collection of integers that is the number of post-shuffle partitions. When not specified, the number of post-shuffle partitions is managed by the link:spark-rdd-Partitioner.adoc[Partitioner] of the input `ShuffleDependency`.

NOTE: *Post-shuffle partition* is...FIXME

.ShuffledRowRDD and RDD Contract
[cols="1,2",options="header",width="100%"]
|===
| Name
| Description

| `getDependencies`
| A single-element collection with `ShuffleDependency[Int, InternalRow, InternalRow]`.

| `partitioner`
| <<CoalescedPartitioner, CoalescedPartitioner>> (with the link:spark-rdd-Partitioner.adoc[Partitioner] of the `dependency`)

| <<getPreferredLocations, getPreferredLocations>>
|

| <<compute, compute>>
|
|===

=== [[numPreShufflePartitions]] `numPreShufflePartitions` Property

CAUTION: FIXME

=== [[compute]] Computing Partition (in `TaskContext`) -- `compute` Method

[source, scala]
----
compute(split: Partition, context: TaskContext): Iterator[InternalRow]
----

NOTE: `compute` is a part of link:spark-rdd.adoc#contract[RDD contract] to compute a given partition in a link:spark-taskscheduler-taskcontext.adoc[TaskContext].

Internally, `compute` makes sure that the input `split` is a <<ShuffledRowRDDPartition, ShuffledRowRDDPartition>>. It then link:spark-ShuffleManager.adoc#contract[requests `ShuffleManager` for a `ShuffleReader`] to read ``InternalRow``s for the `split`.

NOTE: `compute` uses link:spark-sparkenv.adoc#shuffleManager[`SparkEnv` to access the current `ShuffleManager`].

NOTE: `compute` uses `ShuffleHandle` (of link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency] dependency) and the pre-shuffle start and end partition offsets.

=== [[getPreferredLocations]] Getting Placement Preferences of Partition -- `getPreferredLocations` Method

[source, scala]
----
getPreferredLocations(partition: Partition): Seq[String]
----

NOTE: `getPreferredLocations` is a part of link:spark-rdd.adoc#contract[RDD contract] to specify placement preferences (aka _preferred task locations_), i.e. where tasks should be executed to be as close to the data as possible.

Internally, `getPreferredLocations` requests link:spark-service-MapOutputTrackerMaster.adoc#getPreferredLocationsForShuffle[`MapOutputTrackerMaster` for the preferred locations] of the input `partition` (for the single link:spark-rdd-ShuffleDependency.adoc[ShuffleDependency]).

NOTE: `getPreferredLocations` uses link:spark-sparkenv.adoc#mapOutputTracker[`SparkEnv` to access the current `MapOutputTrackerMaster`] (which runs on the driver).

=== [[CoalescedPartitioner]] `CoalescedPartitioner`

CAUTION: FIXME

=== [[ShuffledRowRDDPartition]] `ShuffledRowRDDPartition`

CAUTION: FIXME
