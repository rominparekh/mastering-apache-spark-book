== [[Builder]] `Builder` -- Building `SparkSession` with Fluent API

`Builder` is the fluent API to build a fully-configured link:spark-sql-sparksession.adoc[SparkSession].

.Builder Methods
[cols="1,2",options="header",width="100%"]
|===
| Method | Description
| <<enableHiveSupport, enableHiveSupport>> | Enables Hive support.
|===

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark: SparkSession = SparkSession.builder
  .appName("My Spark Application")  // optional and will be autogenerated if not specified
  .master("local[*]")               // avoid hardcoding the deployment environment
  .enableHiveSupport()              // self-explanatory, isn't it?
  .getOrCreate
----

You can use the fluent design pattern to set the various properties of a `SparkSession` that opens a session to Spark SQL.

NOTE: You can have multiple ``SparkSession``s in a single Spark application for different link:spark-sql-sparksession.adoc#catalog[data catalogs] (through relational entities).

=== [[config]] `config` method

CAUTION: FIXME

=== [[enableHiveSupport]] Enabling Hive Support -- `enableHiveSupport` Method

When link:spark-sql-sparksession.adoc#creating-instance[creating a `SparkSession`], you can optionally enable Hive support using `enableHiveSupport` method.

[source, scala]
----
enableHiveSupport(): Builder
----

`enableHiveSupport` enables Hive support (with connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions).

[NOTE]
====
You do *not* need any existing Hive installation to use Spark's Hive support. `SparkSession` context will automatically create `metastore_db` in the current directory of a Spark application and a directory configured by link:spark-sql-settings.adoc#spark_sql_warehouse_dir[spark.sql.warehouse.dir].

Refer to link:spark-sql-sparksession.adoc#SharedState[SharedState].
====

Internally, `enableHiveSupport` makes sure that the Hive classes are on CLASSPATH, i.e. Spark SQL's link:spark-sql-queryplanner.adoc#HiveSessionState[org.apache.spark.sql.hive.HiveSessionState] and `org.apache.hadoop.hive.conf.HiveConf`, and sets link:spark-sql-settings.adoc#spark.sql.catalogImplementation[spark.sql.catalogImplementation] property to `hive`.
