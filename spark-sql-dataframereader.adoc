== `DataFrameReader` -- Reading from External Data Sources

`DataFrameReader` is an interface to read data from external data sources, e.g. <<creating-dataframes-from-files, files>>, <<creating-dataframes-from-tables, Hive tables>> or <<jdbc, JDBC>> (including link:spark-sql-thrift-server.adoc[Spark Thrift Server]), into a link:spark-sql-dataframe.adoc[DataFrame].

NOTE: You can link:spark-sql-datasource-custom-formats.adoc[define your own custom file formats].

You use link:spark-sql-sparksession.adoc#read[SparkSession.read] to access an instance of `DataFrameReader`.

[source, scala]
----
val spark: SparkSession = SparkSession.builder.getOrCreate

import org.apache.spark.sql.DataFrameReader
val reader: DataFrameReader = spark.read
----

`DataFrameReader` supports many <<creating-dataframes-from-files, file formats>> and <<format, interface for new ones>>.

NOTE: `DataFrameReader` assumes <<parquet, parquet>> file format by default that you can change using link:spark-sql-settings.adoc#spark.sql.sources.default[spark.sql.sources.default] setting.

As of Spark 2.0, `DataFrameReader` can read text files using <<textFile, textFile>> methods that return `Dataset[String]` (not `DataFrames` which are `Dataset[Row]` and therefore untyped).

After you describe the external data source using ``DataFrameReader``'s methods you can trigger the loading using one of <<load, load>> methods.

[source, scala]
----
spark.read
  .format("csv")
  .option("header", true)
  .option("inferSchema", true)
  .load("*.csv")
----

=== [[format]] Specifying Data Format -- `format` method

[source, scala]
----
format(source: String): DataFrameReader
----

You use `format` to configure `DataFrameReader` to use appropriate `source` format.

Supported data formats:

* `json`
* `csv` (since **2.0.0**)
* `parquet` (see link:spark-parquet.adoc[Parquet])
* `orc`
* `text`
* `jdbc`
* `libsvm` -- only when used in `format("libsvm")`

NOTE: You can improve your understanding of `format("jdbc")` with the exercise link:exercises/spark-exercise-dataframe-jdbc-postgresql.adoc[Creating DataFrames from Tables using JDBC and PostgreSQL].

=== [[schema]] Specifying Input Schema -- `schema` method

[source, scala]
----
schema(schema: StructType): DataFrameReader
----

You can specify a `schema` of the input data source.

TIP: Refer to link:spark-sql-schema.adoc[Schema].

=== [[option]][[options]] Adding Extra Configuration Options -- `option` and `options` methods

[source, scala]
----
option(key: String, value: String): DataFrameReader
option(key: String, value: Boolean): DataFrameReader  // <1>
option(key: String, value: Long): DataFrameReader     // <1>
option(key: String, value: Double): DataFrameReader   // <1>
----
<1> Available since Spark **2.0.0**

You can also use `options` method to describe different options in a single `Map`.

[source, scala]
----
options(options: scala.collection.Map[String, String]): DataFrameReader
----

=== [[load]] Loading Datasets (into `DataFrame`) -- `load` methods

[source, scala]
----
load(): DataFrame
load(path: String): DataFrame
load(paths: String*): DataFrame
----

`load` loads input data as a `DataFrame`.

Internally, `load` link:spark-sql-datasource.adoc#creating-instance[creates a `DataSource`] (for the current link:spark-sql-sparksession.adoc[SparkSession], a user-specified <<schema, schema>>, a source <<format, format>> and <<options, options>>). It then immediately link:spark-sql-datasource.adoc#resolveRelation[resolves it] and link:spark-sql-sparksession.adoc#baseRelationToDataFrame[converts `BaseRelation` into a `DataFrame`].

=== [[creating-dataframes-from-files]] Creating DataFrames from Files

`DataFrameReader` supports the following file formats:

* <<json, JSON>>
* <<csv, CSV>>
* <<parquet, parquet>>
* <<orc, ORC>>
* <<text, text>>

==== [[json]] `json` method

[source, scala]
----
json(path: String): DataFrame
json(paths: String*): DataFrame
json(jsonRDD: RDD[String]): DataFrame
----

New in **2.0.0**: `prefersDecimal`

==== [[csv]] `csv` method

[source, scala]
----
csv(path: String): DataFrame
csv(paths: String*): DataFrame
----

==== [[parquet]] `parquet` method

[source, scala]
----
parquet(path: String): DataFrame
parquet(paths: String*): DataFrame
----

The supported options:

* <<compression, compression>> (default: `snappy`)

New in *2.0.0*: `snappy` is the default Parquet codec. See https://github.com/apache/spark/commit/2f0b882e5c8787b09bedcc8208e6dcc5662dbbab[[SPARK-14482\][SQL\] Change default Parquet codec from gzip to snappy].

[[compression]] The compressions supported:

* `none` or `uncompressed`
* `snappy` - the default codec in Spark *2.0.0*.
* `gzip` - the default codec in Spark before *2.0.0*
* `lzo`

[source, scala]
----
val tokens = Seq("hello", "henry", "and", "harry")
  .zipWithIndex
  .map(_.swap)
  .toDF("id", "token")

val parquetWriter = tokens.write
parquetWriter.option("compression", "none").save("hello-none")

// The exception is mostly for my learning purposes
// so I know where and how to find the trace to the compressions
// Sorry...
scala> parquetWriter.option("compression", "unsupported").save("hello-unsupported")
java.lang.IllegalArgumentException: Codec [unsupported] is not available. Available codecs are uncompressed, gzip, lzo, snappy, none.
  at org.apache.spark.sql.execution.datasources.parquet.ParquetOptions.<init>(ParquetOptions.scala:43)
  at org.apache.spark.sql.execution.datasources.parquet.DefaultSource.prepareWrite(ParquetRelation.scala:77)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$4.apply(InsertIntoHadoopFsRelation.scala:122)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$4.apply(InsertIntoHadoopFsRelation.scala:122)
  at org.apache.spark.sql.execution.datasources.BaseWriterContainer.driverSideSetup(WriterContainer.scala:103)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:141)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:53)
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:116)
  at org.apache.spark.sql.execution.command.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:61)
  at org.apache.spark.sql.execution.command.ExecutedCommand.sideEffectResult(commands.scala:59)
  at org.apache.spark.sql.execution.command.ExecutedCommand.doExecute(commands.scala:73)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:118)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:137)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:134)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:117)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:65)
  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:390)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:230)
  ... 48 elided
----

==== [[orc]] `orc` method

[source, scala]
----
orc(path: String): DataFrame
orc(paths: String*): DataFrame
----

*Optimized Row Columnar (ORC)* file format is a highly efficient columnar format to store Hive data with more than 1,000 columns and improve performance. ORC format was introduced in Hive version 0.11 to use and retain the type information from the table definition.

TIP: Read https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC[ORC Files] document to learn about the ORC file format.

==== [[text]] `text` method

`text` method loads a text file.

[source, scala]
----
text(path: String): DataFrame
text(paths: String*): DataFrame
----

===== [[text-example]] Example

[source, scala]
----
val lines: Dataset[String] = spark.read.text("README.md").as[String]

scala> lines.show
+--------------------+
|               value|
+--------------------+
|      # Apache Spark|
|                    |
|Spark is a fast a...|
|high-level APIs i...|
|supports general ...|
|rich set of highe...|
|MLlib for machine...|
|and Spark Streami...|
|                    |
|<http://spark.apa...|
|                    |
|                    |
|## Online Documen...|
|                    |
|You can find the ...|
|guide, on the [pr...|
|and [project wiki...|
|This README file ...|
|                    |
|   ## Building Spark|
+--------------------+
only showing top 20 rows
----

=== [[creating-dataframes-from-tables]] Creating DataFrames from Tables

==== [[table]] `table` method

[source, scala]
----
table(tableName: String): DataFrame
----

`table` method returns the `tableName` table as a `DataFrame`.

[source, scala]
----
scala> spark.sql("SHOW TABLES").show(false)
+---------+-----------+
|tableName|isTemporary|
+---------+-----------+
|dafa     |false      |
+---------+-----------+

scala> spark.read.table("dafa").show(false)
+---+-------+
|id |text   |
+---+-------+
|1  |swiecie|
|0  |hello  |
+---+-------+
----

CAUTION: FIXME The method uses `spark.sessionState.sqlParser.parseTableIdentifier(tableName)` and `spark.sessionState.catalog.lookupRelation`. Would be nice to learn a bit more on their internals, huh?

==== [[jdbc]] Accessing JDBC Data Sources -- `jdbc` method

NOTE: `jdbc` method uses `java.util.Properties` (and appears so Java-centric). Use <<format, format("jdbc")>> instead.

[source, scala]
----
jdbc(url: String, table: String, properties: Properties): DataFrame
jdbc(url: String, table: String,
  parts: Array[Partition],
  connectionProperties: Properties): DataFrame
jdbc(url: String, table: String,
  predicates: Array[String],
  connectionProperties: Properties): DataFrame
jdbc(url: String, table: String,
  columnName: String,
  lowerBound: Long,
  upperBound: Long,
  numPartitions: Int,
  connectionProperties: Properties): DataFrame
----

`jdbc` allows you to create `DataFrame` that represents `table` in the database available as `url`.

TIP: Review the exercise link:exercises/spark-exercise-dataframe-jdbc-postgresql.adoc[Creating DataFrames from Tables using JDBC and PostgreSQL].

=== [[textFile]] Reading Text Files -- `textFile` methods

[source, scala]
----
textFile(path: String): Dataset[String]
textFile(paths: String*): Dataset[String]
----

`textFile` methods query text files as a `Dataset[String]`.

[source, scala]
----
spark.read.textFile("README.md")
----

NOTE: `textFile` are similar to <<text, text>> family of methods in that they both read text files but `text` methods return untyped `DataFrame` while `textFile` return typed `Dataset[String]`.

Internally, `textFile` passes calls on to <<text, text>> method and link:spark-sql-dataset.adoc#select[selects] the only `value` column before it applies `Encoders.STRING` link:spark-sql-Encoder.adoc[encoder].
