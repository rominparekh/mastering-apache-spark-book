== DataSource API -- Loading and Saving Datasets

=== [[reading-datasets]] Reading Datasets

Spark SQL can read data from external storage systems like files, Hive tables and JDBC databases through link:spark-sql-dataframereader.adoc[DataFrameReader] interface.

You use link:spark-sql-sparksession.adoc[SparkSession] to access `DataFrameReader` using link:spark-sql-sparksession.adoc#read[read] operation.

[source, scala]
----
import org.apache.spark.sql.SparkSession
val spark = SparkSession.builder.getOrCreate

val reader = spark.read
----

`DataFrameReader` is an interface to create link:spark-sql-dataframe.adoc[DataFrames] (aka `Dataset[Row]`) from link:spark-sql-dataframereader.adoc#creating-dataframes-from-files[files], link:spark-sql-dataframereader.adoc#creating-dataframes-from-tables[Hive tables] or link:spark-sql-dataframereader.adoc#jdbc[JDBC].

[source, scala]
----
val people = reader.csv("people.csv")
val cities = reader.format("json").load("cities.json")
----

As of Spark 2.0, `DataFrameReader` can read text files using link:spark-sql-dataframereader.adoc#textFile[textFile] methods that return `Dataset[String]` (not `DataFrames`).

[source, scala]
----
spark.read.textFile("README.md")
----

You can also link:spark-sql-datasource-custom-formats.adoc[define your own custom file formats].

[source, scala]
----
val countries = reader.format("customFormat").load("countries.cf")
----

There are two operation modes in Spark SQL, i.e. batch and link:spark-structured-streaming.adoc[streaming] (part of Spark Structured Streaming).

You can access link:spark-sql-streaming-DataStreamReader.adoc[DataStreamReader] for reading streaming datasets through link:spark-sql-sparksession.adoc#readStream[SparkSession.readStream] method.

[source, scala]
----
import org.apache.spark.sql.streaming.DataStreamReader
val stream: DataStreamReader = spark.readStream
----

The available methods in `DataStreamReader` are similar to `DataFrameReader`.

=== [[saving-datasets]] Saving Datasets

Spark SQL can save data to external storage systems like files, Hive tables and JDBC databases through link:spark-sql-dataframewriter.adoc[DataFrameWriter] interface.

You use link:spark-sql-dataset.adoc#write[write] method on a `Dataset` to access `DataFrameWriter`.

[source, scala]
----
import org.apache.spark.sql.{DataFrameWriter, Dataset}
val ints: Dataset[Int] = (0 to 5).toDS

val writer: DataFrameWriter[Int] = ints.write
----

`DataFrameWriter` is an interface to persist a link:spark-sql-dataset.adoc[Datasets] to an external storage system in a batch fashion.

You can access link:spark-sql-streaming-DataStreamWriter.adoc[DataStreamWriter] for writing streaming datasets through link:spark-sql-dataset.adoc#writeStream[Dataset.writeStream] method.

[source, scala]
----
val papers = spark.readStream.text("papers").as[String]

import org.apache.spark.sql.streaming.DataStreamWriter
val writer: DataStreamWriter[String] = papers.writeStream
----

The available methods in `DataStreamWriter` are similar to `DataFrameWriter`.
